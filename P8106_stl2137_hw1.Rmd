---
title: "P8106_stl2137_HW1"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(ModelMetrics)
library(pls)
library(ISLR)

set.seed(1)
```

```{r, echo=FALSE}
training_dat <- read_csv("./data/solubility_train.csv")
test_dat <- read_csv("./data/solubility_test.csv")
```

```{r}
### Creating variables & training control for Linear Model 

# matrix of predictors (training)
## [,-1] due to intercept variable
x <- model.matrix(Solubility ~ ., training_dat)[,-1]

# vector of response (training)
y <- training_dat$Solubility

# creating training controls 
control1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

# matrix of predictors (test)
x.test <- model.matrix(Solubility ~ ., test_dat)[,-1]

# vector of response (test)
y.test <- test_dat$Solubility
```

## Part A

```{r, warning=FALSE}
lm.fit <- train(x, y, 
                method = "lm",
                trControl = control1)

predict.lm.fit <- predict(lm.fit, newdata = test_dat)
linear_mse <- mse(y.test, predict.lm.fit)
```

The MSE of the linear model on the test data is `r linear_mse`.

## Part B 

```{r}
ridge.fit <- train(x, y, 
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(-5, 5, length = 100))),
                   trControl = control1)

plot(ridge.fit, xTrans = function(x) log(x))

ridge.fit$bestTune
#coef(ridge.fit$finalModel, ridge.fit$bestTune$lambda)

predict.ridge.fit <- predict(ridge.fit, newdata = test_dat)
ridge_mse <- mse(y.test, predict.ridge.fit)
```

The MSE of the ridge regression model on the test data is `r ridge_mse`, with the chosen $\lambda$ of `r ridge.fit$bestTune$lambda`. 

## Part C

```{r}
lasso.fit <- train(x, y, 
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(-10, 0, length = 100))),
                   trControl = control1
                   )

plot(lasso.fit, xTrans = function(x) log(x))

lasso.fit$bestTune$lambda

coef_estimates <- coef(lasso.fit$finalModel,lasso.fit$bestTune$lambda) 
num_coef <- sum(as.vector(coef_estimates) != 0)

predict.lasso.fit <- predict(lasso.fit, newdata = test_dat)
lasso_mse <- mse(y.test, predict.lasso.fit)
```

Using a $\lambda$ of `r lasso.fit$bestTune$lambda`, the MSE of the lasso regression on the test data is `r lasso.fit$bestTune$lambda`. There are `r num_coef`non-zero coefficient estimates.

## Part D

```{r}
set.seed(1)
pcr.fit <- train(x, y,
                 method = "pcr",
                 tuneGrid  = data.frame(ncomp = 1:226),
                 trControl = control1,
                 preProc = c("center", "scale"))

trans <- preProcess(x, method = c("center", "scale"))
predy2.pcr2 <- predict(pcr.fit$finalModel, newdata = predict(trans, x.test), 
                       ncomp = pcr.fit$bestTune$ncomp)
pcr_mse <- mse(y.test, predy2.pcr2)

ggplot(pcr.fit, highlight = TRUE) + theme_bw()
```

The MSE of the model using PCR on the test data is `r pcr_mse`, with M equal to 149.

## Part E

```{r}
mse_table <- tibble(
  model = c("Linear", "Ridge", "Lasso", "PCR"),
  mse = c(linear_mse, ridge_mse, lasso_mse, pcr_mse)
)
```

Based off the table of MSE's derived from each model, we can see that lasso has the lowest mse. 

## Part F

```{r}
resamp <- resamples(list(lasso = lasso.fit, 
                         ridge = ridge.fit, 
                         pcr = pcr.fit, 
                         lm = lm.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")

parallelplot(resamp, metric = "RMSE")
```

Based off the MSE, box plot, and RMSE summary, I would use the lasso model for predicting purposes, as it has the lowest MSE out of all the models. 

